{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4159eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg')\n",
    "\n",
    "from src.models.monai_unet_2d import build_monai_unet_2d\n",
    "from src.data.dataset_cases import MicroUSCaseDataset\n",
    "from src.data.transforms_2d import center_crop_or_pad_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bf169",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DIR = Path(\"/home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/runs/20260216_173816\")\n",
    "CKPT_PATH = RUN_DIR / \"checkpoint_best.pt\"\n",
    "\n",
    "DATA_ROOT = Path(\"/home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/dataset/processed/Dataset120_MicroUSProstate\")\n",
    "SPLITS_DIR = Path(\"/home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/dataset/splits\")\n",
    "CASE_STATS_PATH = DATA_ROOT / \"case_stats.json\"\n",
    "\n",
    "# Match your 2D training target size\n",
    "TARGET_HW = (896, 1408)\n",
    "\n",
    "# IMPORTANT:\n",
    "# If you trained 2D with transpose_hw=True, set this True.\n",
    "# If you trained with transpose_hw=False, set this False.\n",
    "TRANSPOSE_HW = True\n",
    "\n",
    "# How many slices to save: center +/- K\n",
    "K_AROUND_CENTER = 1  # saves 3 slices total (center-1, center, center+1)\n",
    "\n",
    "\n",
    "def _overlay(img: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Simple overlay (no fancy colors): brighten masked region.\"\"\"\n",
    "    img01 = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "    out = img01.copy()\n",
    "    out[mask > 0.5] = np.clip(out[mask > 0.5] * 0.6 + 0.4, 0, 1)\n",
    "    return out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def main() -> None:\n",
    "    if not CKPT_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Missing checkpoint: {CKPT_PATH}\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "\n",
    "    # Try to auto-detect model variant from checkpoint extra\n",
    "    variant = \"base\"\n",
    "    extra = ckpt.get(\"extra\", {})\n",
    "    if isinstance(extra, dict):\n",
    "        args = extra.get(\"args\", {})\n",
    "        if isinstance(args, dict) and \"model_variant\" in args:\n",
    "            variant = args[\"model_variant\"]\n",
    "\n",
    "    print(\"Model variant:\", variant)\n",
    "\n",
    "    model, _meta = build_monai_unet_2d(in_channels=1, out_channels=1, variant=variant)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load one validation case (full volume)\n",
    "    val_ds = MicroUSCaseDataset(\n",
    "        dataset_root=str(DATA_ROOT),\n",
    "        splits_dir=str(SPLITS_DIR),\n",
    "        split=\"val\",\n",
    "        use_case_stats=True,\n",
    "        case_stats_path=str(CASE_STATS_PATH),\n",
    "    )\n",
    "\n",
    "    sample = val_ds[0]\n",
    "    case_id = sample[\"case_id\"]\n",
    "    img3d = sample[\"image\"]  # [1, Z, Y, X]\n",
    "    lbl3d = sample[\"label\"]  # [1, Z, Y, X]\n",
    "\n",
    "    # convert to numpy for per-slice operations\n",
    "    img3d_np = img3d.squeeze(0).numpy()  # [Z, Y, X]\n",
    "    lbl3d_np = lbl3d.squeeze(0).numpy()  # [Z, Y, X]\n",
    "\n",
    "    Z = img3d_np.shape[0]\n",
    "    center = Z // 2\n",
    "\n",
    "    out_dir = RUN_DIR / \"viz_2d\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Case: {case_id} | volume shape: {tuple(img3d_np.shape)} | saving to: {out_dir}\")\n",
    "\n",
    "    # choose slice indices to visualize\n",
    "    slice_ids = [s for s in range(center - K_AROUND_CENTER, center + K_AROUND_CENTER + 1) if 0 <= s < Z]\n",
    "\n",
    "    for s in slice_ids:\n",
    "        img2d = img3d_np[s]  # [Y, X]\n",
    "        lbl2d = lbl3d_np[s]  # [Y, X]\n",
    "\n",
    "        if TRANSPOSE_HW:\n",
    "            img2d = img2d.T\n",
    "            lbl2d = lbl2d.T\n",
    "\n",
    "        # match 2D validation preprocessing: deterministic center crop/pad to TARGET_HW\n",
    "        img2d = center_crop_or_pad_2d(img2d, TARGET_HW, pad_value=0.0)\n",
    "        lbl2d = center_crop_or_pad_2d(lbl2d, TARGET_HW, pad_value=0.0)\n",
    "\n",
    "        # to torch: [1,1,H,W]\n",
    "        x = torch.from_numpy(img2d).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "        y = torch.from_numpy(lbl2d).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "        logits = model(x)  # [1,1,H,W]\n",
    "        prob = torch.sigmoid(logits)[0, 0].detach().cpu().numpy()\n",
    "        pred = (prob > 0.5).astype(np.float32)\n",
    "\n",
    "        # Save images\n",
    "        base = out_dir / f\"{case_id}_slice{s:03d}\"\n",
    "\n",
    "        # image\n",
    "        plt.figure()\n",
    "        plt.imshow(img2d, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{case_id} slice {s} image\")\n",
    "        plt.savefig(str(base) + \"_img.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "        # label\n",
    "        plt.figure()\n",
    "        plt.imshow(lbl2d, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{case_id} slice {s} label\")\n",
    "        plt.savefig(str(base) + \"_lbl.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "        # pred\n",
    "        plt.figure()\n",
    "        plt.imshow(pred, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{case_id} slice {s} pred@0.5\")\n",
    "        plt.savefig(str(base) + \"_pred.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "        # overlay\n",
    "        ov = _overlay(img2d.astype(np.float32), pred)\n",
    "        plt.figure()\n",
    "        plt.imshow(ov, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{case_id} slice {s} overlay\")\n",
    "        plt.savefig(str(base) + \"_overlay.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Saved: {base}_*.png\", flush=True)\n",
    "\n",
    "    print(\"Done.\", flush=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Model variant: base\n",
      "Case: microUS_46 | volume shape: (39, 962, 1372) | saving to: /home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/runs/20260216_173816/viz_2d\n",
      "Slices: [19, 20, 24, 31]\n",
      "Saved: /home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/runs/20260216_173816/viz_2d/microUS_46_slice019_dice0.871.png\n",
      "Saved: /home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/runs/20260216_173816/viz_2d/microUS_46_slice020_dice0.850.png\n",
      "Saved: /home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/runs/20260216_173816/viz_2d/microUS_46_slice024_dice0.940.png\n",
      "Saved: /home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/runs/20260216_173816/viz_2d/microUS_46_slice031_dice0.798.png\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg\")\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.monai_unet_2d import build_monai_unet_2d\n",
    "from src.data.dataset_cases import MicroUSCaseDataset\n",
    "from src.data.transforms_2d import center_crop_or_pad_2d  # adjust import if your path differs\n",
    "\n",
    "# -------------------------\n",
    "# EDIT THESE\n",
    "# -------------------------\n",
    "RUN_DIR = Path(\"/home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/runs/20260216_173816\")\n",
    "CKPT_PATH = RUN_DIR / \"checkpoint_best.pt\"\n",
    "\n",
    "DATA_ROOT = Path(\"/home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/dataset/processed/Dataset120_MicroUSProstate\")\n",
    "SPLITS_DIR = Path(\"/home/pirie03/projects/aip-medilab/pirie03/ProstateMicroSeg/dataset/splits\")\n",
    "CASE_STATS_PATH = DATA_ROOT / \"case_stats.json\"\n",
    "\n",
    "# Match your 2D training target size\n",
    "TARGET_HW = (896, 1408)\n",
    "\n",
    "# If you trained 2D with transpose_hw=True, set this True.\n",
    "TRANSPOSE_HW = False\n",
    "\n",
    "# Picks 3 random slices + 1 center slice\n",
    "N_RANDOM_SLICES = 3\n",
    "SEED = 0\n",
    "\n",
    "# Threshold for binarizing prediction\n",
    "THR = 0.5\n",
    "\n",
    "\n",
    "def _overlay(img: np.ndarray, mask: np.ndarray, alpha: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Overlay mask in yellow on grayscale image.\n",
    "    \"\"\"\n",
    "    # Normalize image to [0,1]\n",
    "    img01 = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "\n",
    "    # Convert to RGB\n",
    "    rgb = np.stack([img01, img01, img01], axis=-1)  # [H,W,3]\n",
    "\n",
    "    # Yellow color\n",
    "    yellow = np.array([1.0, 1.0, 0.0])\n",
    "\n",
    "    # Apply overlay where mask > 0.5\n",
    "    mask_bool = mask > 0.5\n",
    "    rgb[mask_bool] = (\n",
    "        (1 - alpha) * rgb[mask_bool] + alpha * yellow\n",
    "    )\n",
    "\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def _dice_binary(pred: np.ndarray, gt: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"Dice for binary 2D masks (0/1).\"\"\"\n",
    "    pred = (pred > 0.5).astype(np.uint8)\n",
    "    gt = (gt > 0.5).astype(np.uint8)\n",
    "    inter = int((pred & gt).sum())\n",
    "    denom = int(pred.sum()) + int(gt.sum())\n",
    "    return float((2.0 * inter + eps) / (denom + eps))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def main() -> None:\n",
    "    if not CKPT_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Missing checkpoint: {CKPT_PATH}\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "\n",
    "    # Try to auto-detect model variant from checkpoint extra\n",
    "    variant = \"base\"\n",
    "    extra = ckpt.get(\"extra\", {})\n",
    "    if isinstance(extra, dict):\n",
    "        args = extra.get(\"args\", {})\n",
    "        if isinstance(args, dict) and \"model_variant\" in args:\n",
    "            variant = args[\"model_variant\"]\n",
    "\n",
    "    print(\"Model variant:\", variant)\n",
    "\n",
    "    model, _meta = build_monai_unet_2d(in_channels=1, out_channels=1, variant=variant)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load one validation case (full volume)\n",
    "    val_ds = MicroUSCaseDataset(\n",
    "        dataset_root=str(DATA_ROOT),\n",
    "        splits_dir=str(SPLITS_DIR),\n",
    "        split=\"val\",\n",
    "        use_case_stats=True,\n",
    "        case_stats_path=str(CASE_STATS_PATH),\n",
    "    )\n",
    "\n",
    "    sample = val_ds[0]\n",
    "    case_id = sample[\"case_id\"]\n",
    "    img3d = sample[\"image\"]  # [1, Z, Y, X]\n",
    "    lbl3d = sample[\"label\"]  # [1, Z, Y, X]\n",
    "\n",
    "    img3d_np = img3d.squeeze(0).numpy()  # [Z, Y, X]\n",
    "    lbl3d_np = lbl3d.squeeze(0).numpy()  # [Z, Y, X]\n",
    "\n",
    "    Z = img3d_np.shape[0]\n",
    "    center = Z // 2\n",
    "\n",
    "    out_dir = RUN_DIR / \"viz_2d\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Case: {case_id} | volume shape: {tuple(img3d_np.shape)} | saving to: {out_dir}\")\n",
    "\n",
    "    # Choose slice indices: center + 3 random (unique, try to avoid center)\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    all_ids = np.arange(Z)\n",
    "    non_center = all_ids[all_ids != center]\n",
    "\n",
    "    if len(non_center) >= N_RANDOM_SLICES:\n",
    "        random_ids = rng.choice(non_center, size=N_RANDOM_SLICES, replace=False).tolist()\n",
    "    else:\n",
    "        # edge case: very short volume, just take what exists\n",
    "        random_ids = non_center.tolist()\n",
    "\n",
    "    slice_ids = sorted(set([center] + random_ids))\n",
    "    print(\"Slices:\", slice_ids)\n",
    "\n",
    "    for s in slice_ids:\n",
    "        img2d = img3d_np[s]  # [Y, X]\n",
    "        lbl2d = lbl3d_np[s]  # [Y, X]\n",
    "\n",
    "        if TRANSPOSE_HW:\n",
    "            img2d = img2d.T\n",
    "            lbl2d = lbl2d.T\n",
    "\n",
    "        # Match 2D validation preprocessing: deterministic center crop/pad to TARGET_HW\n",
    "        img2d = center_crop_or_pad_2d(img2d, TARGET_HW, pad_value=0.0)\n",
    "        lbl2d = center_crop_or_pad_2d(lbl2d, TARGET_HW, pad_value=0.0)\n",
    "\n",
    "        # to torch: [1,1,H,W]\n",
    "        x = torch.from_numpy(img2d).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "        logits = model(x)  # [1,1,H,W]\n",
    "        prob = torch.sigmoid(logits)[0, 0].detach().cpu().numpy()\n",
    "        pred = (prob > THR).astype(np.float32)\n",
    "\n",
    "        dice = _dice_binary(pred, lbl2d)\n",
    "\n",
    "        # 2x2 figure: label, prediction, image+label overlay, image+pred overlay\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "        fig.suptitle(f\"{case_id} | slice {s} | Dice={dice:.3f} (thr={THR})\", fontsize=12)\n",
    "\n",
    "        axes[0, 0].imshow(lbl2d, cmap=\"gray\")\n",
    "        axes[0, 0].set_title(\"Label\")\n",
    "        axes[0, 0].axis(\"off\")\n",
    "\n",
    "        axes[0, 1].imshow(pred, cmap=\"gray\")\n",
    "        axes[0, 1].set_title(\"Prediction\")\n",
    "        axes[0, 1].axis(\"off\")\n",
    "\n",
    "        axes[1, 0].imshow(_overlay(img2d.astype(np.float32), lbl2d), cmap=\"gray\")\n",
    "        axes[1, 0].set_title(\"Image + Label overlay\")\n",
    "        axes[1, 0].axis(\"off\")\n",
    "\n",
    "        axes[1, 1].imshow(_overlay(img2d.astype(np.float32), pred), cmap=\"gray\")\n",
    "        axes[1, 1].set_title(\"Image + Pred overlay\")\n",
    "        axes[1, 1].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_path = out_dir / f\"{case_id}_slice{s:03d}_dice{dice:.3f}.png\"\n",
    "        fig.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved: {out_path}\", flush=True)\n",
    "\n",
    "    print(\"Done.\", flush=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad729c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prostate_microseg (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
